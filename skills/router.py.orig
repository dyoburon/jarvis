import asyncio
import inspect
import json
import logging

from google import genai
from google.genai import types
from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel

import config

_log = logging.getLogger("jarvis.router")
_log.setLevel(logging.DEBUG)
_fh = logging.FileHandler("/tmp/jarvis_router.log")
_fh.setFormatter(logging.Formatter("[%(asctime)s] %(message)s", datefmt="%H:%M:%S"))
_log.addHandler(_fh)
from connectors.token_tracker import TokenTracker
from skills.code_assistant import CODE_SYSTEM_PROMPT, CODE_TOOLS
from skills.code_tools import TOOL_DISPATCH
from skills.domains import DomainSkill
from skills.firewall import FirewallSkill
from skills.papers import PaperSkill
from skills.vibetotext import VibeToTextSkill

console = Console()

# Gemini-format tool declarations for the default conversation session
DEFAULT_TOOLS = [
    types.Tool(function_declarations=[
        types.FunctionDeclaration(
            name="get_domain_dashboard",
            description="Get today's domain drop hunting results: matched domains, zone stats, disappeared domains",
            parameters_json_schema={
                "type": "object",
                "properties": {
                    "date": {"type": "string", "description": "Date (YYYY-MM-DD), defaults to today"},
                    "min_score": {"type": "number", "description": "Minimum match score filter"},
                },
            },
        ),
        types.FunctionDeclaration(
            name="get_paper_dashboard",
            description="Get today's research paper matches from arXiv, grouped by interest area",
            parameters_json_schema={
                "type": "object",
                "properties": {
                    "date": {"type": "string", "description": "Date (YYYY-MM-DD), defaults to today"},
                    "interest": {"type": "string", "description": "Filter to specific research interest"},
                },
            },
        ),
        types.FunctionDeclaration(
            name="get_firewall_status",
            description="Get chat moderation stats from the Great Firewall: recent messages, blocked count",
            parameters_json_schema={"type": "object", "properties": {}},
        ),
        types.FunctionDeclaration(
            name="get_vibetotext_stats",
            description="Get voice transcription statistics: total words dictated, sessions, WPM",
            parameters_json_schema={
                "type": "object",
                "properties": {
                    "limit": {"type": "integer", "description": "Number of recent entries"},
                },
            },
        ),
        types.FunctionDeclaration(
            name="get_system_overview",
            description="Get a full overview across all connected systems",
            parameters_json_schema={"type": "object", "properties": {}},
        ),
        types.FunctionDeclaration(
            name="code_assistant",
            description="Help with coding tasks: read/write/edit files, run commands, search code. Use when the user asks about code, wants to make changes to projects, run scripts, debug issues, or explore codebases.",
            parameters_json_schema={
                "type": "object",
                "properties": {
                    "task": {"type": "string", "description": "What the user wants to do"},
                    "project": {"type": "string", "description": "Project name or directory if specified"},
                },
                "required": ["task"],
            },
        ),
    ])
]

DEFAULT_SYSTEM_PROMPT = (
    config.SYSTEM_PROMPT + "\n\n"
    "You have tools to check connected systems (domains, papers, firewall, vibetotext, system overview) "
    "and a code assistant for coding tasks. Use them when the user asks about these topics. "
    "For casual conversation, just respond with text."
)

# Skill instances
_skills = {
    "get_domain_dashboard": DomainSkill(),
    "get_paper_dashboard": PaperSkill(),
    "get_firewall_status": FirewallSkill(),
    "get_vibetotext_stats": VibeToTextSkill(),
}


# Register data skill executors so Gemini can call them as tools
def _make_data_executor(skill):
    async def executor(**params):
        return await skill.fetch_data(**params)
    return executor


for _name, _skill in _skills.items():
    TOOL_DISPATCH[_name] = _make_data_executor(_skill)


async def _system_overview_executor(**params):
    all_data = {}
    for name, skill in _skills.items():
        try:
            all_data[skill.name] = await skill.fetch_data()
        except Exception as e:
            all_data[skill.name] = {"error": str(e)}
    return all_data


TOOL_DISPATCH["get_system_overview"] = _system_overview_executor


class SkillRouter:
    def __init__(self, metal_bridge=None):
        self.gemini = genai.Client(api_key=config.GOOGLE_API_KEY)
        self.metal = metal_bridge
        self.token_tracker = TokenTracker(config.TOKEN_USAGE_DB)
        # Default conversation session (Gemini Flash)
        self.default_chat = None
        # Streaming chat session for skill mode
        self.active_chat = None
        self.active_skill_name: str | None = None
        self.active_chat_is_code: bool = False
        self.cancelled = False  # set True to abort _run_code_turn
        self._pending_approval: asyncio.Future | None = None
        self._pending_command: str | None = None
        # Session-level token/cost accumulators
        self._session_prompt_tokens: int = 0
        self._session_completion_tokens: int = 0
        self._session_cost: float = 0.0
        self._session_model: str = ""

    def _metal_hud(self, text: str):
        if self.metal:
            self.metal.send_hud(text)

    def _metal_state(self, state: str, name: str = None):
        if self.metal:
            self.metal.send_state(state, name)

    def _record_usage(self, chunk, model: str, session_type: str):
        """Extract usage_metadata from final stream chunk and record it."""
        if chunk is None:
            return
        meta = getattr(chunk, "usage_metadata", None)
        if meta is None:
            return
        prompt = meta.prompt_token_count or 0
        completion = meta.candidates_token_count or 0
        total = meta.total_token_count or 0
        self.token_tracker.record(
            model=model,
            session_type=session_type,
            prompt_tokens=prompt,
            completion_tokens=completion,
            total_tokens=total,
        )
        # Accumulate session totals
        self._session_prompt_tokens += prompt
        self._session_completion_tokens += completion
        self._session_model = model
        pricing = config.GEMINI_PRICING.get(model, {"input": 0, "output": 0})
        self._session_cost += (prompt * pricing["input"] + completion * pricing["output"]) / 1_000_000

    # ── Default conversation (Gemini Flash) ──

    def start_default_session(self):
        """Create the persistent Gemini Flash chat for default conversation."""
        self.default_chat = self.gemini.aio.chats.create(
            model=config.GEMINI_MODEL_DEFAULT,
            config=types.GenerateContentConfig(
                system_instruction=DEFAULT_SYSTEM_PROMPT,
                tools=DEFAULT_TOOLS,
            ),
        )
        console.print("[green]Default Gemini Flash session ready[/]")

    async def send_default_message(self, user_text: str) -> tuple[str, dict | None]:
        """Send message to default Flash session with tool-calling loop.

        Returns (response_text, skill_trigger_or_None).
        If a skill trigger is returned, the caller should enter skill mode.
        skill_trigger = {"tool_name": str, "arguments": str, "user_text": str}
        """
        if not self.default_chat:
            self.start_default_session()

        full_response = ""
        message = user_text
        max_tool_calls = 3
        max_iterations = 3
        total_tool_calls = 0

        for _ in range(max_iterations):
            turn_text = ""
            pending_function_calls = []

            try:
                stream = await asyncio.wait_for(
                    self.default_chat.send_message_stream(message),
                    timeout=30.0,
                )
                last_chunk = None
                async for chunk in stream:
                    last_chunk = chunk
                    if chunk.text:
                        turn_text += chunk.text
                    if chunk.candidates:
                        for candidate in chunk.candidates:
                            if candidate.content and candidate.content.parts:
                                for part in candidate.content.parts:
                                    if part.function_call:
                                        pending_function_calls.append(part.function_call)
                self._record_usage(last_chunk, config.GEMINI_MODEL_DEFAULT, "default")
            except asyncio.TimeoutError:
                console.print("[yellow]Default session timed out (30s)[/]")
                full_response += turn_text + "\n*(Timed out.)*"
                break

            full_response += turn_text

            if not pending_function_calls:
                break

            # Process function calls
            function_response_parts = []
            for fc in pending_function_calls:
                tool_name = fc.name
                tool_args = dict(fc.args) if fc.args else {}
                total_tool_calls += 1

                # code_assistant → signal skill mode (don't execute here)
                if tool_name == "code_assistant":
                    console.print(f"\n[bold yellow]Skill triggered:[/] {tool_name}")
                    return full_response, {
                        "tool_name": tool_name,
                        "arguments": json.dumps(tool_args),
                        "user_text": user_text,
                    }

                # Data tools → execute inline
                console.print(f"  [dim]Default tool: {tool_name}[/]")
                executor = TOOL_DISPATCH.get(tool_name)
                if executor:
                    try:
                        if asyncio.iscoroutinefunction(executor):
                            result = await executor(**tool_args)
                        else:
                            result = executor(**tool_args)
                    except Exception as e:
                        result = {"error": str(e)}
                else:
                    result = {"error": f"Unknown tool: {tool_name}"}

                function_response_parts.append(
                    types.Part.from_function_response(name=tool_name, response=result)
                )

                if total_tool_calls >= max_tool_calls:
                    break

            if not function_response_parts:
                break

            message = function_response_parts

        return full_response, None

    # ── Skill sessions ──

    async def start_skill_session(self, tool_name: str, arguments: str, user_transcript: str, on_chunk=None, on_tool_activity=None) -> str:
        """Start a streaming skill chat session. Returns the full initial response."""
        if tool_name == "code_assistant":
            return await self._start_code_session(arguments, user_transcript, on_chunk, on_tool_activity)

        params = json.loads(arguments) if arguments else {}

        if tool_name == "get_system_overview":
            all_data = {}
            for name, skill in _skills.items():
                try:
                    all_data[skill.name] = await skill.fetch_data()
                except Exception as e:
                    all_data[skill.name] = {"error": str(e)}
            skill_name = "System Overview"
            prompt = (
                "You are Jarvis, reporting on all connected systems.\n"
                f"User asked: \"{user_transcript}\"\n\n"
                f"Data:\n{json.dumps(all_data, indent=2, default=str)}\n\n"
                "Provide a detailed analysis. Use bullet points. Be specific with numbers."
            )
        else:
            skill = _skills.get(tool_name)
            if not skill:
                return f"Unknown skill: {tool_name}"
            skill_name = skill.name
            data = await skill.fetch_data(**params)
            if "error" in data:
                return data["error"]
            prompt = skill.format_prompt(data, user_transcript or tool_name)

        self.active_skill_name = skill_name
        console.print(f"  [dim]Starting Gemini chat for {skill_name}...[/]")

        self.active_chat = self.gemini.aio.chats.create(
            model=config.GEMINI_MODEL_DEFAULT,
            config={"system_instruction": (
                "You are Jarvis, a personal AI assistant. You are in a text chat window "
                "displayed on screen. The user can see your responses as text. "
                "Be detailed but well-formatted. Use bullet points and short paragraphs. "
                "The user may ask follow-up questions about the data.\n\n"
                "When data has interesting patterns, include a visualization using a "
                "fenced code block with language 'chart' containing JSON:\n"
                "```chart\n"
                '{"type":"bar","title":"Chart Title","labels":["A","B","C"],"values":[10,20,30]}\n'
                "```\n"
                "Supported types: bar, line, pie. Keep labels short. "
                "Always include text analysis alongside charts, never just a chart alone."
            )},
        )

        full_response = ""
        try:
            stream = await asyncio.wait_for(
                self.active_chat.send_message_stream(prompt),
                timeout=60.0,
            )
            last_chunk = None
            async for chunk in stream:
                last_chunk = chunk
                text = chunk.text
                if text:
                    full_response += text
                    if on_chunk:
                        on_chunk(text)
            self._record_usage(last_chunk, config.GEMINI_MODEL_DEFAULT, "skill")
        except asyncio.TimeoutError:
            console.print("[yellow]Gemini request timed out (60s)[/]")
            if on_chunk:
                on_chunk("\n\n*(Request timed out.)*")

        return full_response

    async def send_followup(self, user_text: str, on_chunk=None, on_tool_activity=None) -> str:
        """Send a follow-up message in the active chat session."""
        if not self.active_chat:
            return "No active chat session"

        if self.active_chat_is_code:
            try:
                return await self._run_code_turn(user_text, on_chunk, on_tool_activity)
            except Exception as e:
                if "function response turn" in str(e).lower():
                    console.print("  [yellow]Resetting code session after interrupted tool loop[/]")
                    self.active_chat = self.gemini.aio.chats.create(
                        model=config.GEMINI_MODEL_CODE,
                        config=types.GenerateContentConfig(
                            system_instruction=CODE_SYSTEM_PROMPT,
                            tools=CODE_TOOLS,
                        ),
                    )
                    return await self._run_code_turn(user_text, on_chunk, on_tool_activity)
                raise

        full_response = ""
        try:
            stream = await asyncio.wait_for(
                self.active_chat.send_message_stream(user_text),
                timeout=60.0,
            )
            last_chunk = None
            async for chunk in stream:
                last_chunk = chunk
                text = chunk.text
                if text:
                    full_response += text
                    if on_chunk:
                        on_chunk(text)
            self._record_usage(last_chunk, config.GEMINI_MODEL_DEFAULT, "skill")
        except asyncio.TimeoutError:
            console.print("[yellow]Gemini followup timed out (60s)[/]")
            if on_chunk:
                on_chunk("\n\n*(Request timed out.)*")

        return full_response

    def approve_command(self, approved: bool):
        """Resolve a pending run_command approval."""
        if self._pending_approval and not self._pending_approval.done():
            self._pending_approval.set_result(approved)

    def close_session(self) -> str:
        """Close the active chat session."""
        name = self.active_skill_name or "Skill"
        self.active_chat = None
        self.active_skill_name = None
        self.active_chat_is_code = False
        self._session_prompt_tokens = 0
        self._session_completion_tokens = 0
        self._session_cost = 0.0
        self._session_model = ""
        return f"{name} session closed."

    def get_session_status(self) -> str:
        """Format status line for the chat window."""
        model = self._session_model or config.GEMINI_MODEL_DEFAULT
        # Session tokens (current session only)
        session_total = self._session_prompt_tokens + self._session_completion_tokens
        if session_total >= 1_000_000:
            tokens_str = f"{session_total / 1_000_000:.1f}M"
        elif session_total >= 1_000:
            tokens_str = f"{session_total / 1_000:.1f}K"
        else:
            tokens_str = str(session_total)
        # Cumulative cost from DB (persists across restarts)
        db_totals = self.token_tracker.get_totals()
        total_prompt = db_totals["total_prompt"]
        total_completion = db_totals["total_completion"]
        cumulative_cost = 0.0
        for row in self.token_tracker.get_by_model():
            pricing = config.GEMINI_PRICING.get(row["model"], {"input": 0, "output": 0})
            cumulative_cost += (row["prompt_tokens"] * pricing["input"] + row["completion_tokens"] * pricing["output"]) / 1_000_000
        if cumulative_cost >= 1.0:
            cost_str = f"${cumulative_cost:.2f}"
        else:
            cost_str = f"${cumulative_cost:.4f}"
        return f"{model} | {tokens_str} tokens | {cost_str}"

    async def start_code_session_idle(self, arguments: str, user_transcript: str):
        """Initialize a code session without sending any message yet."""
        self.active_skill_name = "Code Assistant"
        self.active_chat_is_code = True
        console.print(f"  [dim]Starting Gemini code session...[/]")

        self.active_chat = self.gemini.aio.chats.create(
            model=config.GEMINI_MODEL_CODE,
            config=types.GenerateContentConfig(
                system_instruction=CODE_SYSTEM_PROMPT,
                tools=CODE_TOOLS,
            ),
        )

    async def send_code_initial(self, user_text: str, on_chunk=None, on_tool_activity=None) -> str:
        """Send the first message to an already-initialized code session."""
        prompt = f"User request: {user_text}"
        return await self._run_code_turn(prompt, on_chunk, on_tool_activity)

    async def _start_code_session(self, arguments: str, user_transcript: str, on_chunk=None, on_tool_activity=None) -> str:
        """Start a coding agent chat session with function-calling tools."""
        params = json.loads(arguments) if arguments else {}
        task = params.get("task", user_transcript)
        project = params.get("project", "")

        self.active_skill_name = "Code Assistant"
        self.active_chat_is_code = True
        console.print(f"  [dim]Starting Gemini code session...[/]")

        self.active_chat = self.gemini.aio.chats.create(
            model=config.GEMINI_MODEL_CODE,
            config=types.GenerateContentConfig(
                system_instruction=CODE_SYSTEM_PROMPT,
                tools=CODE_TOOLS,
            ),
        )

        prompt = f"User request: {task}"
        if project:
            prompt += f"\nProject: {project}"
        prompt += f'\n\nOriginal voice transcript: "{user_transcript}"'

        return await self._run_code_turn(prompt, on_chunk, on_tool_activity)

    async def _run_code_turn(self, message, on_chunk=None, on_tool_activity=None) -> str:
        """Execute the agentic tool-calling loop.

        Streams text to on_chunk. When function calls are detected, executes
        them, notifies on_tool_activity, sends results back to Gemini, and
        repeats until Gemini returns a text-only response.
        """
        self.cancelled = False
        full_response = ""
        total_tool_calls = 0
        max_tool_calls = 30
        max_iterations = 15

        for iteration in range(max_iterations):
            if self.cancelled:
                _log.debug("turn %d: cancelled before start", iteration + 1)
                break

            turn_text = ""
            pending_function_calls = []

            try:
                if not self.active_chat:
                    _log.warning("turn %d: no active chat", iteration + 1)
                    console.print("[yellow]No active chat — loop exiting[/]")
                    break

                stream_timeout = 300.0  # 5 min total time for this turn
                turn_start = asyncio.get_event_loop().time()
                _log.debug("turn %d: sending to Gemini...", iteration + 1)

                stream = await asyncio.wait_for(
                    self.active_chat.send_message_stream(message),
                    timeout=30.0,
                )
                _log.debug("turn %d: stream opened, reading chunks...", iteration + 1)
                last_chunk = None
                chunk_count = 0
                async for chunk in stream:
                    chunk_count += 1
                    # Check total turn timeout
                    elapsed = asyncio.get_event_loop().time() - turn_start
                    if elapsed > stream_timeout:
                        _log.warning("turn %d: stream exceeded %.0fs after %d chunks", iteration + 1, stream_timeout, chunk_count)
                        console.print(f"[yellow]Stream exceeded {stream_timeout}s — stopping[/]")
                        if on_chunk:
                            on_chunk("\n\n*(Stream timed out.)*")
                        break

                    last_chunk = chunk
                    if self.cancelled:
                        _log.debug("turn %d: cancelled mid-stream at chunk %d", iteration + 1, chunk_count)
                        break
                    if chunk.text:
                        turn_text += chunk.text
                        if on_chunk:
                            on_chunk(chunk.text)
                    if chunk.candidates:
                        for candidate in chunk.candidates:
                            if candidate.content and candidate.content.parts:
                                for part in candidate.content.parts:
                                    if part.function_call:
                                        pending_function_calls.append(part.function_call)

                elapsed = asyncio.get_event_loop().time() - turn_start
                _log.debug("turn %d: stream done — %d chunks, %d func calls, %.1fs, text=%d chars",
                           iteration + 1, chunk_count, len(pending_function_calls), elapsed, len(turn_text))
                self._record_usage(last_chunk, config.GEMINI_MODEL_CODE, "code")
            except asyncio.TimeoutError:
                _log.warning("turn %d: TIMEOUT waiting for Gemini", iteration + 1)
                console.print(f"[yellow]Gemini request timed out (turn {iteration + 1})[/]")
                if on_chunk:
                    on_chunk("\n\n*(Request timed out.)*")
                break
            except Exception as e:
                if self.cancelled:
                    _log.debug("turn %d: exception after cancel: %s", iteration + 1, e)
                    break
                _log.error("turn %d: stream error: %s", iteration + 1, e, exc_info=True)
                console.print(f"[red]Stream error (turn {iteration + 1}):[/] {e}")
                if on_chunk:
                    on_chunk(f"\n\n*(Error: {e})*")
                break

            if self.cancelled:
                break

            full_response += turn_text

            if not pending_function_calls:
                _log.debug("turn %d: no function calls — done", iteration + 1)
                break

            remaining = max_tool_calls - total_tool_calls
            if remaining <= 0:
                if on_chunk:
                    on_chunk("\n\n*(Tool limit reached — ask me to continue if needed.)*")
                break
            pending_function_calls = pending_function_calls[:remaining]

            function_response_parts = []
            for fc in pending_function_calls:
                if self.cancelled:
                    break

                tool_name = fc.name
                tool_args = dict(fc.args) if fc.args else {}
                total_tool_calls += 1

                if on_tool_activity:
                    on_tool_activity("start", tool_name, tool_args)

                # Gate: require user approval for run_command
                if tool_name == "run_command":
                    self._pending_command = tool_args.get("command", "")
                    self._pending_approval = asyncio.get_event_loop().create_future()
                    if on_tool_activity:
                        on_tool_activity("approval_request", tool_name, tool_args)
                    try:
                        approved = await self._pending_approval
                    except (asyncio.CancelledError, Exception):
                        self._pending_approval = None
                        self._pending_command = None
                        break
                    self._pending_approval = None
                    self._pending_command = None
                    if not approved:
                        result = {"error": "Command denied by user."}
                        if on_tool_activity:
                            on_tool_activity("result", tool_name, result)
                        console.print(f"  [red]Command denied[/]")
                        function_response_parts.append(
                            types.Part.from_function_response(name=tool_name, response=result)
                        )
                        continue

                _log.debug("tool %d/%d: executing %s", total_tool_calls, max_tool_calls, tool_name)
                executor = TOOL_DISPATCH.get(tool_name)
                if executor:
                    try:
                        if asyncio.iscoroutinefunction(executor):
                            result = await asyncio.wait_for(executor(**tool_args), timeout=45.0)
                        else:
                            result = executor(**tool_args)
                    except asyncio.TimeoutError:
                        _log.warning("tool %s timed out (45s)", tool_name)
                        result = {"error": f"Tool {tool_name} timed out (45s)"}
                    except Exception as e:
                        _log.error("tool %s error: %s", tool_name, e)
                        result = {"error": str(e)}
                else:
                    result = {"error": f"Unknown tool: {tool_name}"}

                has_error = "error" in result if isinstance(result, dict) else False
                _log.debug("tool %s done — error=%s", tool_name, has_error)

                if on_tool_activity:
                    on_tool_activity("result", tool_name, result)

                console.print(f"  [dim]Tool {tool_name} ({total_tool_calls}/{max_tool_calls})[/]")
                function_response_parts.append(
                    types.Part.from_function_response(name=tool_name, response=result)
                )

            if self.cancelled or not function_response_parts:
                _log.debug("turn %d: loop ending — cancelled=%s, parts=%d",
                           iteration + 1, self.cancelled, len(function_response_parts))
                break

            message = function_response_parts

        _log.debug("_run_code_turn done — %d turns, %d tool calls, response=%d chars",
                    iteration + 1, total_tool_calls, len(full_response))
        return full_response
